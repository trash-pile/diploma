{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91f8f6-1bbc-4733-bf15-8ffba9d17297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting image stitching process from 3 to 36\n",
      "\n",
      "Processing image 4/35 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_26252\\2971160060.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.last_match_region = (int(transformed_center[0]), int(transformed_center[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8s]\n",
      "\n",
      "Processing image 5/35 [2.1s]\n",
      "\n",
      "Processing image 6/35 [2.2s]\n",
      "\n",
      "Processing image 7/35 [2.2s]\n",
      "\n",
      "Processing image 8/35 [2.4s]\n",
      "\n",
      "Processing image 9/35 [2.4s]\n",
      "\n",
      "Processing image 10/35 [2.4s]\n",
      "\n",
      "Processing image 11/35 [2.6s]\n",
      "\n",
      "Processing image 12/35 [2.4s]\n",
      "\n",
      "Processing image 13/35 [2.5s]\n",
      "\n",
      "Processing image 14/35 [2.5s]\n",
      "\n",
      "Processing image 15/35 "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from time import time\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    KEEP_PERCENT: float = 0.75\n",
    "    THRESH: float = 250.0\n",
    "    NFEAT: int = 100000\n",
    "    WIDEN: int = 4000\n",
    "    SCALE: float = 0.15\n",
    "    MATCH_RATIO: float = 0.7\n",
    "    MIN_INLIERS: float = 0.01\n",
    "    # Search window size in pixels (after scaling)\n",
    "    SEARCH_WINDOW: int = 1000\n",
    "    # Overlap margin to ensure we don't miss matches\n",
    "    OVERLAP_MARGIN: int = 200\n",
    "\n",
    "class ImageStitcher:\n",
    "    def __init__(self, path: str = \"./folder2/\"):\n",
    "        self.path = path\n",
    "        self.sift = cv2.SIFT_create(nfeatures=Config.NFEAT)\n",
    "        self.matcher = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n",
    "        self.last_match_region = None\n",
    "\n",
    "    def read_image(self, idx: int) -> np.ndarray:\n",
    "        img = cv2.imread(f\"{self.path}2023_09_01_SonyRX1RM2_g201b20538_f001_{idx:04}.JPG\")\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Cannot read image {idx}\")\n",
    "        \n",
    "        img = cv2.resize(img, None, fx=Config.SCALE, fy=Config.SCALE)\n",
    "        rgba = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
    "        rgba[:, :, 3] = 255\n",
    "        return rgba\n",
    "\n",
    "    def get_search_region(self, img: np.ndarray) -> tuple:\n",
    "        \"\"\"Determine the search region based on the last successful match\"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if self.last_match_region is None:\n",
    "            # For the first match, use the right half of the image\n",
    "            x1 = max(0, w // 2 - Config.OVERLAP_MARGIN)\n",
    "            y1 = 0\n",
    "            x2 = w\n",
    "            y2 = h\n",
    "        else:\n",
    "            # Use the region around the last match\n",
    "            x1 = max(0, self.last_match_region[0] - Config.SEARCH_WINDOW // 2)\n",
    "            y1 = max(0, self.last_match_region[1] - Config.SEARCH_WINDOW // 2)\n",
    "            x2 = min(w, x1 + Config.SEARCH_WINDOW)\n",
    "            y2 = min(h, y1 + Config.SEARCH_WINDOW)\n",
    "            \n",
    "            # Add overlap margin\n",
    "            x1 = max(0, x1 - Config.OVERLAP_MARGIN)\n",
    "            y1 = max(0, y1 - Config.OVERLAP_MARGIN)\n",
    "            x2 = min(w, x2 + Config.OVERLAP_MARGIN)\n",
    "            y2 = min(h, y2 + Config.OVERLAP_MARGIN)\n",
    "        \n",
    "        return (x1, y1, x2, y2)\n",
    "\n",
    "    def update_last_match_region(self, matrix: np.ndarray, img_shape: tuple):\n",
    "        \"\"\"Update the last match region based on the transformation matrix\"\"\"\n",
    "        # Calculate the center point of the transformed image\n",
    "        h, w = img_shape[:2]\n",
    "        center = np.array([[w/2, h/2, 1]], dtype=np.float32).T\n",
    "        transformed_center = matrix @ center\n",
    "        \n",
    "        self.last_match_region = (int(transformed_center[0]), int(transformed_center[1]))\n",
    "\n",
    "    def find_matches(self, img1: np.ndarray, img2: np.ndarray):\n",
    "        # Get the search region for the first image\n",
    "        x1, y1, x2, y2 = self.get_search_region(img1)\n",
    "        \n",
    "        # Extract the region of interest from img1\n",
    "        img1_roi = img1[y1:y2, x1:x2]\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray1 = cv2.cvtColor(img1_roi, cv2.COLOR_BGRA2GRAY)\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGRA2GRAY)\n",
    "        \n",
    "        # Detect features\n",
    "        kp1, desc1 = self.sift.detectAndCompute(gray1, None)\n",
    "        kp2, desc2 = self.sift.detectAndCompute(gray2, None)\n",
    "        \n",
    "        if len(kp1) == 0 or len(kp2) == 0:\n",
    "            raise ValueError(\"No features detected in one or both images\")\n",
    "        \n",
    "        # Match features\n",
    "        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n",
    "        good = [m for m, n in matches if m.distance < Config.MATCH_RATIO * n.distance]\n",
    "        good = sorted(good, key=lambda x: x.distance)[:int(len(good) * Config.KEEP_PERCENT)]\n",
    "        \n",
    "        if len(good) < 4:\n",
    "            raise ValueError(\"Not enough good matches found\")\n",
    "        \n",
    "        # Adjust keypoint coordinates for the ROI offset\n",
    "        pts1 = np.float32([kp1[m.queryIdx].pt for m in good])\n",
    "        pts1 += [x1, y1]  # Add offset to coordinates\n",
    "        pts2 = np.float32([kp2[m.trainIdx].pt for m in good])\n",
    "        \n",
    "        return pts1, pts2\n",
    "\n",
    "    def align_images(self, pts1: np.ndarray, pts2: np.ndarray, img_shape: tuple):\n",
    "        matrix, inliers = cv2.estimateAffinePartial2D(pts2, pts1, method=cv2.RANSAC,\n",
    "                                                     ransacReprojThreshold=Config.THRESH)\n",
    "        \n",
    "        if matrix is None or np.count_nonzero(inliers) < Config.MIN_INLIERS * len(pts1):\n",
    "            raise ValueError(\"Not enough matches\")\n",
    "        \n",
    "        # Update the last match region\n",
    "        self.update_last_match_region(matrix, img_shape)\n",
    "        \n",
    "        return matrix\n",
    "\n",
    "    def widen_image(self, img: np.ndarray) -> np.ndarray:\n",
    "        h, w = img.shape[:2]\n",
    "        pad = int(Config.WIDEN * Config.SCALE)\n",
    "        canvas = np.zeros((h + pad, w + pad, 4), dtype=np.uint8)\n",
    "        offset = pad // 2\n",
    "        canvas[offset:offset + h, offset:offset + w] = img\n",
    "        return canvas\n",
    "\n",
    "    def blend_images(self, img1: np.ndarray, img2: np.ndarray) -> np.ndarray:\n",
    "        result = np.zeros_like(img1)\n",
    "        a1 = img1[:, :, 3].astype(float) / 255\n",
    "        a2 = img2[:, :, 3].astype(float) / 255\n",
    "        a_out = a2 + a1 * (1 - a2)\n",
    "        mask = a_out > 0\n",
    "\n",
    "        for c in range(3):\n",
    "            result[mask, c] = (img2[mask, c] * a2[mask] + \n",
    "                             img1[mask, c] * a1[mask] * (1 - a2[mask])) / a_out[mask]\n",
    "        \n",
    "        result[:, :, 3] = (a_out * 255).astype(np.uint8)\n",
    "        return result\n",
    "\n",
    "    def crop_result(self, img: np.ndarray) -> np.ndarray:\n",
    "        coords = np.argwhere(img[:, :, 3] > 0)\n",
    "        if len(coords) == 0:\n",
    "            return img\n",
    "        y1, x1 = coords.min(axis=0)\n",
    "        y2, x2 = coords.max(axis=0) + 1\n",
    "        return img[y1:y2, x1:x2].copy()\n",
    "\n",
    "    def stitch(self, start: int = 3, end: int = 36):\n",
    "        print(f\"\\nStarting image stitching process from {start} to {end}\")\n",
    "        start_time = time()\n",
    "        result = self.read_image(start)\n",
    "        total_images = end - start - 1\n",
    "        processed = 0\n",
    "        failures = 0\n",
    "        \n",
    "        for idx in range(start + 1, end):\n",
    "            try:\n",
    "                iter_start = time()\n",
    "                print(f\"\\nProcessing image {idx}/{end-1} \", end=\"\")\n",
    "                \n",
    "                # Read and prepare images\n",
    "                current = self.read_image(idx)\n",
    "                current = self.widen_image(current)\n",
    "                result = self.widen_image(result)\n",
    "\n",
    "                # Align images\n",
    "                pts1, pts2 = self.find_matches(result, current)\n",
    "                matrix = self.align_images(pts1, pts2, current.shape)\n",
    "                \n",
    "                # Warp and blend\n",
    "                aligned = cv2.warpAffine(current, matrix, (result.shape[1], result.shape[0]),\n",
    "                                       flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_TRANSPARENT)\n",
    "                result = self.blend_images(result, aligned)\n",
    "                result = self.crop_result(result)\n",
    "\n",
    "                # Save intermediate results\n",
    "                if idx == 35:\n",
    "                    cv2.imwrite(f\"result_{idx}.png\", result)\n",
    "                    print(f\"[Saved result_{idx}.png]\", end=\"\")\n",
    "                \n",
    "                processed += 1\n",
    "                print(f\"[{time() - iter_start:.1f}s]\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                failures += 1\n",
    "                print(f\"\\nError at image {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        total_time = time() - start_time\n",
    "        print(f\"\\nStitching completed:\")\n",
    "        print(f\"Total images processed: {processed}/{total_images} ({failures} failed)\")\n",
    "        print(f\"Total time: {total_time:.1f}s (avg {total_time/total_images:.1f}s per image)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stitcher = ImageStitcher()\n",
    "    stitcher.stitch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e1fb6-d61f-4767-b143-992122ff19ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
