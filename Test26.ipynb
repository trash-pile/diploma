{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d4ffd-5b40-4970-a681-3f4c86a99eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 worker threads (all available cores)\n",
      "Search field area multiplier: 1.6x each dimension (2.56x total area)\n",
      "\n",
      "Starting image stitching process from 3 to 100\n",
      "  - Read initial image: 0.328s\n",
      "    - Construct path: 0.000s\n",
      "    - Read from disk: 0.277s\n",
      "    - Resize: 0.020s\n",
      "    - Convert to RGBA: 0.031s\n",
      "  - Expand initial image: 0.000s\n",
      "\n",
      "Processing image 4/100\n",
      "  - Find matches: 11.577s\n",
      "    - Extract ROI: 0.000s\n",
      "    - Allocate buffers: 0.000s\n",
      "    - Convert to grayscale: 0.016s\n",
      "    - SIFT detection: 9.768s\n",
      "    - FLANN matching: 1.709s\n",
      "    - Filter matches: 0.008s\n",
      "    - Extract points: 0.053s\n",
      "  - Align images: 0.010s\n",
      "  - Warp and overlay: 1.647s\n",
      "  - Crop result: 0.040s\n",
      "  - Update match region: 0.005s\n",
      "[Total for image 4: 13.573s]\n",
      "\n",
      "Processing image 5/100\n",
      "  - Find matches: 12.188s\n",
      "    - Extract ROI: 0.000s\n",
      "    - Allocate buffers: 0.000s\n",
      "    - Convert to grayscale: 0.023s\n",
      "    - SIFT detection: 10.224s\n",
      "    - FLANN matching: 1.719s\n",
      "    - Filter matches: 0.009s\n",
      "    - Extract points: 0.187s\n",
      "  - Align images: 0.011s\n",
      "  - Warp and overlay: 0.120s\n",
      "  - Crop result: 0.047s\n",
      "  - Update match region: 0.004s\n",
      "[Total for image 5: 12.372s]\n",
      "\n",
      "Processing image 6/100\n",
      "  - Find matches: 9.657s\n",
      "    - Extract ROI: 0.000s\n",
      "    - Allocate buffers: 0.000s\n",
      "    - Convert to grayscale: 0.021s\n",
      "    - SIFT detection: 7.887s\n",
      "    - FLANN matching: 1.669s\n",
      "    - Filter matches: 0.007s\n",
      "    - Extract points: 0.047s\n",
      "  - Align images: 0.000s\n",
      "  - Warp and overlay: 0.102s\n",
      "  - Crop result: 0.037s\n",
      "  - Update match region: 0.000s\n",
      "[Total for image 6: 9.797s]\n",
      "\n",
      "Processing image 7/100\n",
      "  - Find matches: 10.143s\n",
      "    - Extract ROI: 0.000s\n",
      "    - Allocate buffers: 0.000s\n",
      "    - Convert to grayscale: 0.012s\n",
      "    - SIFT detection: 8.344s\n",
      "    - FLANN matching: 1.695s\n",
      "    - Filter matches: 0.007s\n",
      "    - Extract points: 0.050s\n",
      "  - Align images: 0.001s\n",
      "  - Warp and overlay: 0.115s\n",
      "  - Crop result: 0.044s\n",
      "  - Update match region: 0.000s\n",
      "[Total for image 7: 10.304s]\n",
      "\n",
      "Processing image 8/100\n",
      "  - Find matches: 10.144s\n",
      "    - Extract ROI: 0.000s\n",
      "    - Allocate buffers: 0.000s\n",
      "    - Convert to grayscale: 0.010s\n",
      "    - SIFT detection: 8.341s\n",
      "    - FLANN matching: 1.703s\n",
      "    - Filter matches: 0.007s\n",
      "    - Extract points: 0.048s\n",
      "  - Align images: 0.000s\n",
      "  - Warp and overlay: 0.110s\n",
      "  - Crop result: 0.042s\n",
      "  - Update match region: 0.000s\n",
      "[Total for image 8: 10.297s]\n",
      "\n",
      "Processing image 9/100\n",
      "  - Find matches: 9.872s\n",
      "    - Extract ROI: 0.000s\n",
      "    - Allocate buffers: 0.000s\n",
      "    - Convert to grayscale: 0.011s\n",
      "    - SIFT detection: 8.070s\n",
      "    - FLANN matching: 1.696s\n",
      "    - Filter matches: 0.007s\n",
      "    - Extract points: 0.047s\n",
      "  - Align images: 0.000s\n",
      "  - Warp and overlay: 0.110s\n",
      "  - Crop result: 0.041s\n",
      "  - Update match region: 0.000s\n",
      "[Total for image 9: 10.024s]\n",
      "\n",
      "Processing image 10/100\n",
      "  - Find matches: 9.659s\n",
      "    - Extract ROI: 0.000s\n",
      "    - Allocate buffers: 0.000s\n",
      "    - Convert to grayscale: 0.011s\n",
      "    - SIFT detection: 7.877s\n",
      "    - FLANN matching: 1.674s\n",
      "    - Filter matches: 0.007s\n",
      "    - Extract points: 0.048s\n",
      "  - Align images: 0.000s\n",
      "  - Warp and overlay: 0.133s\n",
      "  - Crop result: 0.050s\n",
      "  - Update match region: 0.000s\n",
      "[Total for image 10: 9.843s]\n",
      "  - Saved result_10.png\n",
      "\n",
      "Processing image 11/100\n",
      "  - Find matches: 9.430s\n",
      "    - Extract ROI: 0.000s\n",
      "    - Allocate buffers: 0.000s\n",
      "    - Convert to grayscale: 0.011s\n",
      "    - SIFT detection: 7.613s\n",
      "    - FLANN matching: 1.700s\n",
      "    - Filter matches: 0.007s\n",
      "    - Extract points: 0.048s\n",
      "  - Align images: 0.000s\n",
      "  - Warp and overlay: 0.118s\n",
      "  - Crop result: 0.044s\n",
      "  - Update match region: 0.000s\n",
      "[Total for image 11: 9.593s]\n",
      "\n",
      "Processing image 12/100\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from time import time\n",
    "from typing import Tuple, Optional\n",
    "from pathlib import Path\n",
    "import numba\n",
    "from concurrent.futures import ThreadPoolExecutor, Future\n",
    "import os\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    SCALE: float = 0.4\n",
    "    NFEAT: int = 50000  # Reduced from 150000 to 50000 for faster feature detection\n",
    "    MATCH_RATIO: float = 0.65\n",
    "    KEEP_PERCENT: float = 0.55\n",
    "    THRESH: float = 250.0\n",
    "    SIFT_EDGE_THRESHOLD: float = 10.0\n",
    "    SIFT_CONTRAST_THRESHOLD: float = 0.04\n",
    "    NUM_THREADS: int = os.cpu_count() if os.cpu_count() else 4\n",
    "    SEARCH_FIELD_MULTIPLIER: float = 1.6  # Multiplier for search field size in each dimension\n",
    "                                          # Results in (SEARCH_FIELD_MULTIPLIER^2)x larger search area\n",
    "                                          # Also determines maximum canvas expansion when needed\n",
    "                                          # Must be >= 1.0\n",
    "\n",
    "class ImageStitcher:\n",
    "    def __init__(self, path: str = \"./folder2/\"):\n",
    "        self.path = Path(path)\n",
    "        \n",
    "        # Validate SEARCH_FIELD_MULTIPLIER\n",
    "        if Config.SEARCH_FIELD_MULTIPLIER < 1.0:\n",
    "            raise ValueError(\"SEARCH_FIELD_MULTIPLIER must be >= 1.0\")\n",
    "            \n",
    "        self.sift = cv2.SIFT_create(\n",
    "            nfeatures=Config.NFEAT,\n",
    "            contrastThreshold=Config.SIFT_CONTRAST_THRESHOLD,\n",
    "            edgeThreshold=Config.SIFT_EDGE_THRESHOLD\n",
    "        )\n",
    "        self.matcher = cv2.FlannBasedMatcher(\n",
    "            dict(algorithm=1, trees=5),\n",
    "            dict(checks=32)\n",
    "        )\n",
    "        self.last_match_region: Optional[Tuple[int, int]] = None\n",
    "        self._gray_buffer1: Optional[np.ndarray] = None\n",
    "        self._gray_buffer2: Optional[np.ndarray] = None\n",
    "        self._base_image_size: Optional[Tuple[int, int]] = None  # Store base image size after scaling\n",
    "        self._canvas_offset: Tuple[int, int] = (0, 0)  # Track cumulative offset from expansions\n",
    "        print(f\"Using {Config.NUM_THREADS} worker threads (all available cores)\")\n",
    "        print(f\"Search field area multiplier: {Config.SEARCH_FIELD_MULTIPLIER}x each dimension \"\n",
    "              f\"({Config.SEARCH_FIELD_MULTIPLIER**2:.2f}x total area)\")\n",
    "        self.thread_pool = ThreadPoolExecutor(max_workers=Config.NUM_THREADS)\n",
    "\n",
    "    def read_image(self, idx: int) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"\n",
    "        Read and preprocess image, returning image and timing details.\n",
    "        \"\"\"\n",
    "        timings = {}\n",
    "\n",
    "        # Construct file path\n",
    "        path_start = time()\n",
    "        img_path = self.path / f\"2023_09_01_SonyRX1RM2_g201b20538_f001_{idx:04}.JPG\"\n",
    "        timings['construct_path'] = time() - path_start\n",
    "\n",
    "        # Read image from disk\n",
    "        read_start = time()\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        timings['read_from_disk'] = time() - read_start\n",
    "\n",
    "        # Resize image using faster interpolation\n",
    "        resize_start = time()\n",
    "        img = cv2.resize(img, None, fx=Config.SCALE, fy=Config.SCALE,\n",
    "                        interpolation=cv2.INTER_LINEAR)  # Changed from INTER_AREA to INTER_LINEAR\n",
    "        timings['resize'] = time() - resize_start\n",
    "        \n",
    "        # Store base image size if not already set\n",
    "        if self._base_image_size is None:\n",
    "            self._base_image_size = (img.shape[1], img.shape[0])  # (width, height)\n",
    "        \n",
    "        # Convert to RGBA\n",
    "        rgba_start = time()\n",
    "        rgba = np.zeros((img.shape[0], img.shape[1], 4), dtype=np.uint8)\n",
    "        rgba[:, :, :3] = img\n",
    "        rgba[:, :, 3] = 255\n",
    "        timings['convert_to_rgba'] = time() - rgba_start\n",
    "\n",
    "        return rgba, timings\n",
    "\n",
    "    def get_search_region_and_expand(self, img: np.ndarray) -> Tuple[np.ndarray, Tuple[int, int, int, int], Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Calculate search region and expand image if necessary.\n",
    "        Returns expanded image, search region coordinates, and new offset.\n",
    "        Expansion is based on SEARCH_FIELD_MULTIPLIER to ensure consistency.\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if self.last_match_region is None:\n",
    "            self.last_match_region = (w // 2, h // 2)  # Initialize to center\n",
    "            return img, (0, 0, w, h), (0, 0)\n",
    "\n",
    "        # Get base image dimensions after scaling\n",
    "        base_w, base_h = self._base_image_size\n",
    "        \n",
    "        # Calculate desired search window dimensions\n",
    "        search_w = int(round(base_w * Config.SEARCH_FIELD_MULTIPLIER**2))\n",
    "        search_h = int(round(base_h * Config.SEARCH_FIELD_MULTIPLIER**2))\n",
    "        search_w = max(base_w, search_w)  # Ensure search region is at least base size\n",
    "        search_h = max(base_h, search_h)\n",
    "\n",
    "        # Adjust last match region for current canvas offset\n",
    "        cx, cy = self.last_match_region\n",
    "\n",
    "        # Calculate desired search region (before boundary check)\n",
    "        x1 = int(round(cx - search_w / 2))\n",
    "        y1 = int(round(cy - search_h / 2))\n",
    "        x2 = x1 + search_w\n",
    "        y2 = y1 + search_h\n",
    "\n",
    "        # Check if search region extends beyond image boundaries\n",
    "        pad_left = max(0, -x1)\n",
    "        pad_right = max(0, x2 - w)\n",
    "        pad_top = max(0, -y1)\n",
    "        pad_bottom = max(0, y2 - h)\n",
    "\n",
    "        # Maximum allowed expansion based on SEARCH_FIELD_MULTIPLIER\n",
    "        max_pad_w = int(round(base_w * Config.SEARCH_FIELD_MULTIPLIER**2))\n",
    "        max_pad_h = int(round(base_h * Config.SEARCH_FIELD_MULTIPLIER**2))\n",
    "\n",
    "        if pad_left or pad_right or pad_top or pad_bottom:\n",
    "            # Limit padding to maximum allowed\n",
    "            pad_left = min(pad_left, max_pad_w)\n",
    "            pad_right = min(pad_right, max_pad_w)\n",
    "            pad_top = min(pad_top, max_pad_h)\n",
    "            pad_bottom = min(pad_bottom, max_pad_h)\n",
    "\n",
    "            # Create new canvas with padding\n",
    "            new_w = w + pad_left + pad_right\n",
    "            new_h = h + pad_top + pad_bottom\n",
    "            canvas = np.zeros((new_h, new_w, 4), dtype=np.uint8)\n",
    "            canvas[pad_top:pad_top + h, pad_left:pad_left + w] = img\n",
    "            \n",
    "            # Update coordinates for new canvas\n",
    "            x1 += pad_left\n",
    "            x2 += pad_left\n",
    "            y1 += pad_top\n",
    "            y2 += pad_top\n",
    "            \n",
    "            result_img = canvas\n",
    "            offset = (pad_left, pad_top)\n",
    "            \n",
    "            # Update canvas offset\n",
    "            self._canvas_offset = (self._canvas_offset[0] + pad_left, \n",
    "                                 self._canvas_offset[1] + pad_top)\n",
    "        else:\n",
    "            result_img = img\n",
    "            offset = (0, 0)\n",
    "\n",
    "        # Clip search region to image boundaries\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(result_img.shape[1], x2)\n",
    "        y2 = min(result_img.shape[0], y2)\n",
    "\n",
    "        return result_img, (x1, y1, x2, y2), offset\n",
    "\n",
    "    def update_last_match_region(self, matrix: np.ndarray, img_shape: Tuple[int, int], \n",
    "                               offset: Tuple[int, int], crop_offset: Optional[Tuple[int, int]] = None):\n",
    "        h, w = img_shape[:2]\n",
    "        center = np.array([[w/2], [h/2], [1]], dtype=np.float32)\n",
    "        transformed = matrix @ center\n",
    "        \n",
    "        # Adjust for expansion offset\n",
    "        new_cx = int(transformed[0, 0]) + offset[0]\n",
    "        new_cy = int(transformed[1, 0]) + offset[1]\n",
    "        \n",
    "        # Adjust for cropping if applicable\n",
    "        if crop_offset:\n",
    "            new_cx -= crop_offset[0]\n",
    "            new_cy -= crop_offset[1]\n",
    "            \n",
    "        self.last_match_region = (new_cx, new_cy)\n",
    "\n",
    "    def find_matches(self, img1: np.ndarray, img2: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Tuple[int, int], dict]:\n",
    "        \"\"\"\n",
    "        Find matches between images, returning results and timing details.\n",
    "        \"\"\"\n",
    "        timings = {}\n",
    "\n",
    "        # Expand search region (already timed separately in stitch)\n",
    "        img1, (x1, y1, x2, y2), offset = self.get_search_region_and_expand(img1)\n",
    "        \n",
    "        # Verify that slice indices are valid\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            raise ValueError(f\"Invalid search region: x1={x1}, x2={x2}, y1={y1}, y2={y2}\")\n",
    "        \n",
    "        # Extract ROI\n",
    "        roi_start = time()\n",
    "        img1_roi = img1[y1:y2, x1:x2]\n",
    "        timings['extract_roi'] = time() - roi_start\n",
    "\n",
    "        # Allocate grayscale buffers if needed\n",
    "        buffer_start = time()\n",
    "        if (self._gray_buffer1 is None or \n",
    "            self._gray_buffer1.shape != img1_roi.shape[:2]):\n",
    "            self._gray_buffer1 = np.empty(img1_roi.shape[:2], dtype=np.uint8)\n",
    "        if (self._gray_buffer2 is None or \n",
    "            self._gray_buffer2.shape != img2.shape[:2]):\n",
    "            self._gray_buffer2 = np.empty(img2.shape[:2], dtype=np.uint8)\n",
    "        timings['allocate_buffers'] = time() - buffer_start\n",
    "\n",
    "        # Convert to grayscale in parallel\n",
    "        gray_start = time()\n",
    "        futures = [\n",
    "            self.thread_pool.submit(cv2.cvtColor, img1_roi, cv2.COLOR_BGRA2GRAY, \n",
    "                                  dst=self._gray_buffer1),\n",
    "            self.thread_pool.submit(cv2.cvtColor, img2, cv2.COLOR_BGRA2GRAY, \n",
    "                                  dst=self._gray_buffer2)\n",
    "        ]\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "        timings['convert_to_grayscale'] = time() - gray_start\n",
    "\n",
    "        # Detect SIFT features and descriptors in parallel\n",
    "        def detect_compute(img, sift):\n",
    "            return sift.detectAndCompute(img, None)\n",
    "        \n",
    "        sift_start = time()\n",
    "        future_kp1 = self.thread_pool.submit(detect_compute, self._gray_buffer1, self.sift)\n",
    "        future_kp2 = self.thread_pool.submit(detect_compute, self._gray_buffer2, self.sift)\n",
    "        kp1, desc1 = future_kp1.result()\n",
    "        kp2, desc2 = future_kp2.result()\n",
    "        timings['sift_detection'] = time() - sift_start\n",
    "\n",
    "        # Perform FLANN-based matching\n",
    "        flann_start = time()\n",
    "        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n",
    "        timings['flann_matching'] = time() - flann_start\n",
    "\n",
    "        # Filter matches\n",
    "        filter_start = time()\n",
    "        good = [m for m, n in matches if m.distance < Config.MATCH_RATIO * n.distance]\n",
    "        good = sorted(good, key=lambda x: x.distance)[:int(len(good) * Config.KEEP_PERCENT)]\n",
    "        timings['filter_matches'] = time() - filter_start\n",
    "\n",
    "        # Extract points\n",
    "        points_start = time()\n",
    "        query_idx = np.array([m.queryIdx for m in good])\n",
    "        train_idx = np.array([m.trainIdx for m in good])\n",
    "        pts1 = np.float32([kp.pt for kp in kp1])[query_idx] + [x1, y1]\n",
    "        pts2 = np.float32([kp.pt for kp in kp2])[train_idx]\n",
    "        timings['extract_points'] = time() - points_start\n",
    "\n",
    "        return img1, pts1, pts2, offset, timings\n",
    "\n",
    "    def align_images(self, pts1: np.ndarray, pts2: np.ndarray, \n",
    "                    img_shape: Tuple[int, int], offset: Tuple[int, int]) -> np.ndarray:\n",
    "        matrix, _ = cv2.estimateAffinePartial2D(\n",
    "            pts2, pts1,\n",
    "            method=cv2.RANSAC,\n",
    "            ransacReprojThreshold=Config.THRESH,\n",
    "            confidence=0.995,\n",
    "            maxIters=1000\n",
    "        )\n",
    "        return matrix\n",
    "\n",
    "    @staticmethod\n",
    "    @numba.jit(nopython=True, parallel=True)\n",
    "    def overlay_images_numba(base: np.ndarray, overlay: np.ndarray, result: np.ndarray):\n",
    "        h, w = base.shape[:2]\n",
    "        for y in numba.prange(h):\n",
    "            for x in range(w):\n",
    "                if overlay[y, x, 3] > 0:\n",
    "                    result[y, x] = overlay[y, x]\n",
    "                else:\n",
    "                    result[y, x] = base[y, x]\n",
    "\n",
    "    def overlay_images(self, base: np.ndarray, overlay: np.ndarray) -> np.ndarray:\n",
    "        result = np.zeros_like(base, dtype=np.uint8)\n",
    "        ImageStitcher.overlay_images_numba(base, overlay, result)\n",
    "        return result\n",
    "\n",
    "    def crop_result(self, img: np.ndarray) -> Tuple[np.ndarray, Tuple[int, int]]:\n",
    "        mask = img[:, :, 3] > 0\n",
    "        rows = np.any(mask, axis=1)\n",
    "        cols = np.any(mask, axis=0)\n",
    "        y1, y2 = np.where(rows)[0][[0, -1]]\n",
    "        x1, x2 = np.where(cols)[0][[0, -1]]\n",
    "        crop_offset = (x1, y1)\n",
    "        return img[y1:y2+1, x1:x2+1], crop_offset\n",
    "\n",
    "    def stitch(self, start: int = 3, end: int = 100):\n",
    "        \"\"\"\n",
    "        Stitch images from start to end index, with prefetching to overlap I/O and computation.\n",
    "        \"\"\"\n",
    "        print(f\"\\nStarting image stitching process from {start} to {end}\")\n",
    "        start_time = time()\n",
    "        \n",
    "        # Read first image\n",
    "        result, read_timings = self.read_image(start)\n",
    "        print(f\"  - Read initial image: {time() - start_time:.3f}s\")\n",
    "        print(f\"    - Construct path: {read_timings['construct_path']:.3f}s\")\n",
    "        print(f\"    - Read from disk: {read_timings['read_from_disk']:.3f}s\")\n",
    "        print(f\"    - Resize: {read_timings['resize']:.3f}s\")\n",
    "        print(f\"    - Convert to RGBA: {read_timings['convert_to_rgba']:.3f}s\")\n",
    "        \n",
    "        # Expand initial image\n",
    "        expand_start = time()\n",
    "        result, _, offset = self.get_search_region_and_expand(result)\n",
    "        expand_time = time() - expand_start\n",
    "        print(f\"  - Expand initial image: {expand_time:.3f}s\")\n",
    "        \n",
    "        future: Optional[Future] = None\n",
    "        for idx in range(start + 1, end + 1):\n",
    "            iter_start = time()\n",
    "            print(f\"\\nProcessing image {idx}/{end}\")\n",
    "            \n",
    "            # Get current image (from future if available)\n",
    "            if future is not None:\n",
    "                current, read_timings = future.result()\n",
    "            else:\n",
    "                current, read_timings = self.read_image(idx)\n",
    "            \n",
    "            # Prefetch next image if not the last one\n",
    "            if idx < end:\n",
    "                future = self.thread_pool.submit(self.read_image, idx + 1)\n",
    "            \n",
    "            # Find matches\n",
    "            match_start = time()\n",
    "            result, pts1, pts2, offset, match_timings = self.find_matches(result, current)\n",
    "            match_time = time() - match_start\n",
    "            \n",
    "            # Align images\n",
    "            align_start = time()\n",
    "            matrix = self.align_images(pts1, pts2, current.shape, offset)\n",
    "            align_time = time() - align_start\n",
    "            \n",
    "            # Warp and overlay\n",
    "            warp_start = time()\n",
    "            aligned = cv2.warpAffine(\n",
    "                current, matrix,\n",
    "                (result.shape[1], result.shape[0]),\n",
    "                flags=cv2.INTER_LINEAR,\n",
    "                borderMode=cv2.BORDER_TRANSPARENT\n",
    "            )\n",
    "            result = self.overlay_images(result, aligned)\n",
    "            warp_time = time() - warp_start\n",
    "            \n",
    "            # Crop result\n",
    "            crop_start = time()\n",
    "            result, crop_offset = self.crop_result(result)\n",
    "            crop_time = time() - crop_start\n",
    "            \n",
    "            # Update last match region\n",
    "            update_start = time()\n",
    "            self.update_last_match_region(matrix, current.shape, offset, crop_offset)\n",
    "            update_time = time() - update_start\n",
    "            \n",
    "            # Print timings\n",
    "            print(f\"  - Find matches: {match_time:.3f}s\")\n",
    "            print(f\"    - Extract ROI: {match_timings['extract_roi']:.3f}s\")\n",
    "            print(f\"    - Allocate buffers: {match_timings['allocate_buffers']:.3f}s\")\n",
    "            print(f\"    - Convert to grayscale: {match_timings['convert_to_grayscale']:.3f}s\")\n",
    "            print(f\"    - SIFT detection: {match_timings['sift_detection']:.3f}s\")\n",
    "            print(f\"    - FLANN matching: {match_timings['flann_matching']:.3f}s\")\n",
    "            print(f\"    - Filter matches: {match_timings['filter_matches']:.3f}s\")\n",
    "            print(f\"    - Extract points: {match_timings['extract_points']:.3f}s\")\n",
    "            print(f\"  - Align images: {align_time:.3f}s\")\n",
    "            print(f\"  - Warp and overlay: {warp_time:.3f}s\")\n",
    "            print(f\"  - Crop result: {crop_time:.3f}s\")\n",
    "            print(f\"  - Update match region: {update_time:.3f}s\")\n",
    "            print(f\"[Total for image {idx}: {time() - iter_start:.3f}s]\")\n",
    "            \n",
    "            if idx % 10 == 0:\n",
    "                cv2.imwrite(f\"result_{idx}.png\", result)\n",
    "                print(f\"  - Saved result_{idx}.png\")\n",
    "        \n",
    "        total_time = time() - start_time\n",
    "        print(f\"\\nStitching completed in {total_time:.3f}s\")\n",
    "\n",
    "    def __del__(self):\n",
    "        self.thread_pool.shutdown()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stitcher = ImageStitcher()\n",
    "    stitcher.stitch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46970fbf-b511-4beb-909b-6d074f2e98d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
